학습의 자동중단
테스트셋 오차가 줄지 않으면 학습을 멈추게 하는 함수 EarlyStopping()
학습을 계속하게 되면 과적합이 일어날수 있기 때문이다.

15장
선형회귀데이터는 마지막에 activation 함수를 사용하지 않음(참거짓을 구분할 필요가 없어서)
flatten() 데이터 배열을 1차원으로 바꿔 읽기 쉽게 함

16CNN
28*28=784픽셀 , 밝기 정도 -> 0~255
reshape(총 샘플의 수, 1차원 속성의 수)
0~255수를 0~1로 바꾸기 위해서 나누기 255를 함 -> 실수로 먼저 바꾸고 나서 해야함
나온 클래스를 바꾸는거 np_utils.to_categorical(클래스, 클래스의 개수) 원 핫 인코딩이 됌

컨볼루션 신경망(CNN)
입력된 이미지에서 다시 한번 특징을 추출하기 위해 커널을 도입하는 기법

맥스풀링
풀링 : 결과가 크고 복잡하면 이를 다시 한번 축소하기 위해서 사용한다.
정해진 구역 안에서 최댓값을 뽑아내는 것

드롭아웃
은닉층에 배치된 노드 중 일부를 임의로 끄는 것
model.add(Dropout(0.25)) -> 25% 끄는 것
주의점 : 컨볼루션 층 or 맥스 풀링은 2차원 배열인채 다루는데 배열을 1차로 바꿔야 사용가능하다

17 자연어 처리

텍스트의 토큰화
단어별, 문장별, 형태소별로 나누는데 나누어진 하나의 단위를 토큰
입력된 텍스트를 잘게 나누는 과정이 토큰화
text_to_word_sequence() -> 토큰화 하는 함수
Tokenizer() -> 단어의 빈도 수를 계산하는 함수

단어의 원-핫 인코딩
맨 앞에는 0인 인덱스가 추가되고 단어는 배열형식으로 해당위치를 1로 표시한다. 
Embedding(단어 수, 벡터크기) 유사도를 계산하는 학습
긍정 부정(1과0)
pad_sequnce() 길이를 동일하게 맞춰주는 작업 패딩

18 순환 신경망(RNN)
여러 개의 데이터가 순서대로 입력되었을 때 앞서 입력받은 데이터를 잠시 기억하는 방법
그리고 별도의 가중치를 줘서 다음 데이터로 넘긴다.
개선한 것이 LSTM을 함께 사용하는 것
한 층 안에서 반복을 많이 하는 RNN의 특성상 일반 신경망보다 기울기 소실 문제가 더 많이 발생하고 
이를 해결하기 어렵다는 단점을 보완

RNN의 장점
입력 값과 출력 값을 어떻게 설정하느냐에 따라 여러 가지 상황에서 적용할 수 있다.

LSTM을 이용한 뉴스 카테고리 분류
reuter.load_data() 기사를 불러옴
test_split 테스트셋 설정
num_word = 1000 -> 1~1000까지 단어 사용
기사의 단어 수가 다르므로 데이터 전처리 함수 sequence()를 이용
maxlen=100 단어 수를 100개까지
LSTM의 활성화 함수 tanh

LSTM+CNN

19장 오토인코더
생산적 적대 신경망 = GAN
진짜 같은 가짜를 만들기 위해 GAN 알고리즘 내부에서는 적대적인 경합 진행
생성자 : 가짜를 만드는 파트
판별자 : 진위를 가려내는 파트
DCGAN : 페이스북 AI 연구팀 개발 CNN을 GAN에 적용한 알고리즘 : 불안정하던 초기의 GAN을 보완했다.
DCGAN의 CNN과 16장의 CNN의 차이점
-Optimizer를 사용하는 최적화 과정이나 컴파일 과정이 없음
-일부 매개 변수를 삭제하는 풀링과정이 없음
-패딩과정을 포함한다. 입력 크기와 출력 크기를 똑같이 맞추기 위함
	padding = 'same' 설정 : 입력과 출력 크기가 다를 경우 크기를 확장하고 확장된 공간에 0을 채워 넣음
-배치 정규화 : 데이터의 배치를 정규분포로 만든다. 입력 데이터의 평균이 0, 분산이 1이 되도록 재배치
생성자의 활성화 함수 : 렐루 함수 사용 판변자로 넘겨주기 직전에는 tanh() 사용

128(노드수)*7*7(이미지크기 28*28이지만 UpSampling2D를 2번 지나면서 28이 됨) 

stride : 가로 세로 크기가 더 줄어 들어 새로운 특징을 뽑아주는 효과가 생긴다.

오토인코더
입력 데이터의 특징을 효율적으로 담아낸 이미지를 만들어 낸다.
원리
입력 층보다 적은 수의 노드를 가진 은닉층을 중간에 넣어 줌으로써 차원을 줄여 준다.
이때 소실된 데이터를 복원하기위해 학습을 시작하고 이 과정을 통해 입력 데이터의 특징을 효율적으로 
응축한 새로운 출력이 나오는 원리
인코딩 - 디코딩

20 딥러닝의 성능 극대화하기
지도학습, 비지도 학습 -> 정답 여부
데이터 부풀리기는 학습셋에만 적용하는 것이 좋음 과적합의 위험을 줄일 수 있기 때문이다.

전이학습으로 성능 극대화


YOLO
물체의 종류와 위치를 추측하여 딥러닝 기반의 물체 인식 알고리즘입니다.
v3에 비해 정확도가 많이 개선됌

BOF(Bag of Freebies)
추록 속도는 유지하며 학습 전략을 바꾸거나 학습 비용을 증가시켜 정확도를 높이는 데이터 증강 기법
BOS(Bag of Specials)
추론 비용만 증가시켜서 추론 속도 증가를 통해 정확도를 높이는 기법










