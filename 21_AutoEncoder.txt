from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape
import matplotlib.pyplot as plt
import numpy as np

#MNIST데이터 셋을 불러옵니다.

(X_train, _), (X_test, _) = mnist.load_data()
X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255
X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255

#생성자 모델 생성
autoencoder = Sequential()

# 인코딩
autoencoder.add(Conv2D(16, kernel_size=3, padding='same', input_shape=(28,28,1), activation='relu'))
autoencoder.add(MaxPooling2D(pool_size=2, padding='same'))
autoencoder.add(Conv2D(8, kernel_size=3, activation='relu', padding='same'))
autoencoder.add(MaxPooling2D(pool_size=2, padding='same'))
autoencoder.add(Conv2D(8, kernel_size=3, strides=2, padding='same', activation='relu'))

# 디코딩
autoencoder.add(Conv2D(8, kernel_size=3, padding='same', activation='relu'))
autoencoder.add(UpSampling2D())
autoencoder.add(Conv2D(8, kernel_size=3, padding='same', activation='relu'))
autoencoder.add(UpSampling2D())
autoencoder.add(Conv2D(16, kernel_size=3, activation='relu'))
autoencoder.add(UpSampling2D())
autoencoder.add(Conv2D(1, kernel_size=3, padding='same', activation='sigmoid'))

# 전체 구조를 확인
autoencoder.summary()

# 컴파일 및 학습을 하는 부분입니다.
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
autoencoder.fit(X_train, X_train, epochs=40, batch_size=128, validation_data=(X_test, X_test))

#학습된 결과를 출력하는 부분입니다.
random_test = np.random.randint(X_test.shape[0], size=5)  #테스트할 이미지를 랜덤하게 불러옵니다.
ae_imgs = autoencoder.predict(X_test)  #앞서 만든 오토인코더 모델에 집어 넣습니다.

plt.figure(figsize=(7, 2))  #출력될 이미지의 크기를 정합니다.

for i, image_idx in enumerate(random_test):    #랜덤하게 뽑은 이미지를 차례로 나열합니다.
   ax = plt.subplot(2, 7, i + 1) 
   plt.imshow(X_test[image_idx].reshape(28, 28))  #테스트할 이미지를 먼저 그대로 보여줍니다.
   ax.axis('off')
   ax = plt.subplot(2, 7, 7 + i +1)
   plt.imshow(ae_imgs[image_idx].reshape(28, 28))  #오토인코딩 결과를 다음열에 출력합니다.
   ax.axis('off')
plt.show()

Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_7 (Conv2D)            (None, 28, 28, 16)        160       
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 14, 14, 16)        0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 14, 14, 8)         1160      
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 7, 7, 8)           0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 4, 4, 8)           584       
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 4, 4, 8)           584       
_________________________________________________________________
up_sampling2d_3 (UpSampling2 (None, 8, 8, 8)           0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 8, 8, 8)           584       
_________________________________________________________________
up_sampling2d_4 (UpSampling2 (None, 16, 16, 8)         0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 14, 14, 16)        1168      
_________________________________________________________________
up_sampling2d_5 (UpSampling2 (None, 28, 28, 16)        0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 28, 28, 1)         145       
=================================================================
Total params: 4,385
Trainable params: 4,385
Non-trainable params: 0
_________________________________________________________________
Epoch 1/40
469/469 [==============================] - 77s 164ms/step - loss: 0.3112 - val_loss: 0.1389
Epoch 2/40
469/469 [==============================] - 77s 163ms/step - loss: 0.1342 - val_loss: 0.1197
Epoch 3/40
469/469 [==============================] - 77s 164ms/step - loss: 0.1181 - val_loss: 0.1106
Epoch 4/40
469/469 [==============================] - 77s 164ms/step - loss: 0.1107 - val_loss: 0.1054
Epoch 5/40
469/469 [==============================] - 76s 162ms/step - loss: 0.1058 - val_loss: 0.1022
Epoch 6/40
469/469 [==============================] - 76s 162ms/step - loss: 0.1028 - val_loss: 0.0995
Epoch 7/40
469/469 [==============================] - 76s 162ms/step - loss: 0.1001 - val_loss: 0.0972
Epoch 8/40
469/469 [==============================] - 77s 164ms/step - loss: 0.0982 - val_loss: 0.0955
Epoch 9/40
469/469 [==============================] - 77s 163ms/step - loss: 0.0966 - val_loss: 0.0947
Epoch 10/40
469/469 [==============================] - 76s 163ms/step - loss: 0.0951 - val_loss: 0.0931
Epoch 11/40
469/469 [==============================] - 76s 163ms/step - loss: 0.0940 - val_loss: 0.0920
Epoch 12/40
469/469 [==============================] - 76s 162ms/step - loss: 0.0931 - val_loss: 0.0912
Epoch 13/40
469/469 [==============================] - 76s 162ms/step - loss: 0.0922 - val_loss: 0.0908
Epoch 14/40
469/469 [==============================] - 76s 163ms/step - loss: 0.0914 - val_loss: 0.0903
Epoch 15/40
469/469 [==============================] - 76s 162ms/step - loss: 0.0909 - val_loss: 0.0895
Epoch 16/40
469/469 [==============================] - 76s 162ms/step - loss: 0.0902 - val_loss: 0.0888
Epoch 17/40
469/469 [==============================] - 76s 163ms/step - loss: 0.0898 - val_loss: 0.0885
Epoch 18/40
469/469 [==============================] - 77s 163ms/step - loss: 0.0893 - val_loss: 0.0879
Epoch 19/40
469/469 [==============================] - 77s 164ms/step - loss: 0.0890 - val_loss: 0.0876
Epoch 20/40
469/469 [==============================] - 76s 162ms/step - loss: 0.0886 - val_loss: 0.0873
Epoch 21/40
469/469 [==============================] - 76s 162ms/step - loss: 0.0883 - val_loss: 0.0869
Epoch 22/40
469/469 [==============================] - 76s 163ms/step - loss: 0.0878 - val_loss: 0.0865
Epoch 23/40
469/469 [==============================] - 76s 163ms/step - loss: 0.0876 - val_loss: 0.0871
Epoch 24/40
469/469 [==============================] - 77s 163ms/step - loss: 0.0872 - val_loss: 0.0861
Epoch 25/40
469/469 [==============================] - 76s 163ms/step - loss: 0.0872 - val_loss: 0.0862
Epoch 26/40
469/469 [==============================] - 77s 163ms/step - loss: 0.0867 - val_loss: 0.0855
Epoch 27/40
469/469 [==============================] - 76s 163ms/step - loss: 0.0868 - val_loss: 0.0853
Epoch 28/40
469/469 [==============================] - 77s 164ms/step - loss: 0.0863 - val_loss: 0.0852
Epoch 29/40
469/469 [==============================] - 76s 163ms/step - loss: 0.0863 - val_loss: 0.0856
Epoch 30/40
469/469 [==============================] - 77s 164ms/step - loss: 0.0861 - val_loss: 0.0852
Epoch 31/40
469/469 [==============================] - 76s 162ms/step - loss: 0.0860 - val_loss: 0.0847
Epoch 32/40
469/469 [==============================] - 76s 163ms/step - loss: 0.0861 - val_loss: 0.0845
Epoch 33/40
469/469 [==============================] - 76s 163ms/step - loss: 0.0856 - val_loss: 0.0846
Epoch 34/40
469/469 [==============================] - 76s 163ms/step - loss: 0.0856 - val_loss: 0.0842
Epoch 35/40
469/469 [==============================] - 76s 163ms/step - loss: 0.0854 - val_loss: 0.0841
Epoch 36/40
469/469 [==============================] - 76s 163ms/step - loss: 0.0852 - val_loss: 0.0840
Epoch 37/40
469/469 [==============================] - 77s 163ms/step - loss: 0.0852 - val_loss: 0.0839
Epoch 38/40
469/469 [==============================] - 77s 164ms/step - loss: 0.0851 - val_loss: 0.0840
Epoch 39/40
469/469 [==============================] - 78s 166ms/step - loss: 0.0849 - val_loss: 0.0837
Epoch 40/40
469/469 [==============================] - 78s 167ms/step - loss: 0.0849 - val_loss: 0.0837

