딥러닝 : 규칙을 찾는 것, x가 주어지고 결과인 y값이 있을때 적절한 a,b를 찾는 것


딥러닝을 이해하는데 가장 중요한 수학원리 -> 미분

순간 변화율 -> x의 증가량이 0에 가까울 만큼 아주 작을떄의 순간적인 기울기


선형회귀, 로지스틱 회귀

x값이 1개 단순 선형회귀 / 다중 선형회귀
최소제곱법을 쓰면 a,b를 구할 수 있음

선형회귀
독립 변수x를 사용해 종속 변수 y의 움직임을 예측하고 설명하는 작업

최소 제곱법 -> 일차 함수의 a,b를 바로 구할 수 있다.

오차를 평가하는 방법 -> 평균 제곱근 오차(MSE)
-> 임의의 선을 긋고 평가하여 조금씩 수정해 가는 방법

경사하강법
오차를 비교하여 가장 작은 방향으로 이동하는 것 즉 미분값이 0인 지점을 찾는 것 
학습률 - 이동거리를 정해주는 것, 최적화 과정 중 하나(너무크면 발산함)

로지스틱 회귀
참인지 거짓인지를 구분하는 로지스틱 회귀 - 입력값의 특징을 추출
1과 0 사이를 구반하는 s자 형태의 선을 그어주는 작업 -> 시그모이드 함수

시그모이드 함수
S자 형태로 그려지는 함수 -> 결과값이 0~1 사이이다


//퍼셉트론
신경망을 이루는 가장 중요한 기본단위
입력 값과 활성화 함수를 사용해 출력 값을 다음으로 넘기는 가장 작은 신경망 단위
가중치(a)바이어스(b)
활성화 함수 -> 0과 1을 판단하는 함수(ex시그모이드 함수)
하이퍼블릭 탄젠트 함수 -> -1~1
렐루 함수 : 시그모이드 대안
소프트 플러스 함수 : 렐루 변형

다중 퍼셉트론
XOR문제 해결을 위한 것, 은닉층을 만든다. 

오차역전파
다중 퍼셉트론에서의 최적화 과정
출력층으로부터 하나씩 앞으로 되돌아가며 각 층의 가중치를 수정하는 방법

고급경사 하강법
확률적 경사 하강법 -> 속도의 단점 보안
랜덤하게 추출한 일부 데이터를 사용한다.
중간 결과의 진폭이 크고 불안전 -> 하지만 빠르고 근사한 값을 찾아낸다.

모멘텀(정확도 개선)
경사하강법에 탄력을 더해 주는 것
같은 방향으로 일정한 비율만 수정되게 하는 방법
지그재그로 일어나는 현상이 줄어들고, 관성의 횩과를 낼 수 있다.

원핫인코딩
여러 개의 Y값을 0과 1로 이루어진 형태로 바꿔 주는 기법

소프트맥스
총합이 1인 형태로 바꿔서 계산해 주는 함수

과적합
일정 수준 이상의 예측 정확도를 보이지만, 새로운 데이터에 적용하면 잘 맞지 않는 것
-> 학습셋 + 데이터셋을 구분하여 학습과 테스트를 병행

numpy 수치 계산 라이브러리 / np.loadtxt()
Sequential 딥러닝의 구조를 한층 한층 쉽게 올리게 해줌
Dense 각 층이 제각각 어떤 특성을 가질지 옵션을 설정하는 역할
rmse(x, y) = x, y 자리에 입력해서 평균제곱근을 구함 -> rmse_val() 을 만듬
activation : 다음 층으로 어떻게 값을 넘길지 결정하는 부분 
loss : 오차 값을 추적하는 함수
optimizer : 오차를 어떻게 줄여나갈지 정하는 함수
mean_squared_error
binary_crossentropy
categorical_croccentropy
시그모이드 함수 = y = 1/(1+np.e**-(a * x_data + b))
tf.placeholder("데이터형", "행렬의 차원", '이름')
matmul 행렬의 곱
metrics 모델이 컴파일될 때 모델 수행 결과를 나타내게끔 설정하는 부분
read_csv : 콤마(,)로 구분된 데이터들의 모음 (헤더 없음)
df.info() 전반적인 정보 확인
df.describe() 정보별 특징을 자세히 알때
df[['asd','class']] 특정 컬럼만 볼 때
groupby 어떠한 정보를 기준으로 새 그룹 만들기
matplotlib 그래프 그릴때 사용
plt.figure(figsize=(x,y)) 크기 설정
heatmap 두 항목씩 작을 지은 뒤 어떤 패턴으로 변화하는지 관찰하는 함수
pairplot(df, hue='species') 데이터 전체를 한번에 보는 그래프